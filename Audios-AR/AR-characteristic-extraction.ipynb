{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52611b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Iterator, Dict, List, Final\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.tools.sm_exceptions import InfeasibleTestError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e8b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Audio Files and Extracting AutoReg Features ---\n",
      "Model Order (lags): 12\n",
      "------------------------------------------------------------\n",
      "Processed: 100.0_0_.wav                        | Time:   0.1071s\n",
      "Processed: 100.0_1_.wav                        | Time:   0.1493s\n",
      "Processed: 100.0_2_.wav                        | Time:   0.1681s\n",
      "Processed: 100.0_3_.wav                        | Time:   0.1206s\n",
      "Processed: 100.0_4_.wav                        | Time:   0.1315s\n",
      "Processed: 100.0_5_.wav                        | Time:   0.1578s\n",
      "Processed: 100.1_0_.wav                        | Time:   0.1322s\n",
      "Processed: 100.1_1_.wav                        | Time:   0.1922s\n",
      "Processed: 100.1_2_.wav                        | Time:   0.1242s\n",
      "Processed: 100.1_3_.wav                        | Time:   0.1268s\n",
      "Processed: 100.1_4_.wav                        | Time:   0.1368s\n",
      "Processed: 100.1_5_.wav                        | Time:   0.1642s\n",
      "Processed: 100.2_0_.wav                        | Time:   0.1570s\n",
      "Processed: 100.2_1_.wav                        | Time:   0.1534s\n",
      "Processed: 100.2_2_.wav                        | Time:   0.1256s\n",
      "Processed: 100.2_3_.wav                        | Time:   0.2146s\n",
      "Processed: 100.2_4_.wav                        | Time:   0.1341s\n",
      "Processed: 100.2_5_.wav                        | Time:   0.1719s\n",
      "Processed: 100.3_0_.wav                        | Time:   0.1228s\n",
      "Processed: 100.3_1_.wav                        | Time:   0.1257s\n",
      "Processed: 100.3_2_.wav                        | Time:   0.1183s\n",
      "Processed: 100.3_3_.wav                        | Time:   0.1154s\n",
      "Processed: 100.3_4_.wav                        | Time:   0.1199s\n",
      "Processed: 100.3_5_.wav                        | Time:   0.1180s\n",
      "Processed: 100.4_0_.wav                        | Time:   0.1119s\n",
      "Processed: 100.4_1_.wav                        | Time:   0.1442s\n",
      "Processed: 100.4_2_.wav                        | Time:   0.1527s\n",
      "Processed: 100.4_3_.wav                        | Time:   0.1518s\n",
      "Processed: 100.4_4_.wav                        | Time:   0.1469s\n",
      "Processed: 100.4_5_.wav                        | Time:   0.1222s\n",
      "Processed: 100.5_0_.wav                        | Time:   0.1357s\n",
      "Processed: 100.5_1_.wav                        | Time:   0.1281s\n",
      "Processed: 100.5_2_.wav                        | Time:   0.1137s\n",
      "Processed: 100.5_3_.wav                        | Time:   0.1209s\n",
      "Processed: 100.5_4_.wav                        | Time:   0.1282s\n",
      "Processed: 100.5_5_.wav                        | Time:   0.1656s\n",
      "Processed: 100.6_0_.wav                        | Time:   0.1338s\n",
      "Processed: 100.6_1_.wav                        | Time:   0.1586s\n",
      "Processed: 100.6_2_.wav                        | Time:   0.1240s\n",
      "Processed: 100.6_3_.wav                        | Time:   0.1152s\n",
      "Processed: 100.6_4_.wav                        | Time:   0.1102s\n",
      "Processed: 100.6_5_.wav                        | Time:   0.1630s\n",
      "Processed: 100.7_0_.wav                        | Time:   0.1664s\n",
      "Processed: 100.7_1_.wav                        | Time:   0.1203s\n",
      "Processed: 100.7_2_.wav                        | Time:   0.1324s\n",
      "Processed: 100.7_3_.wav                        | Time:   0.1294s\n",
      "Processed: 100.7_4_.wav                        | Time:   0.1182s\n",
      "Processed: 100.7_5_.wav                        | Time:   0.1506s\n",
      "Processed: 100.8_0_.wav                        | Time:   0.1328s\n",
      "Processed: 100.8_1_.wav                        | Time:   0.1094s\n",
      "Processed: 100.8_2_.wav                        | Time:   0.0992s\n",
      "Processed: 100.8_3_.wav                        | Time:   0.0885s\n",
      "Processed: 100.8_4_.wav                        | Time:   0.1057s\n",
      "Processed: 100.8_5_.wav                        | Time:   0.1068s\n",
      "Processed: 100.9_0_.wav                        | Time:   0.1002s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 212\u001b[39m\n\u001b[32m    209\u001b[39m PATH_TO_AUDIO_DIR = \u001b[33m\"\u001b[39m\u001b[33m/home/javastral/GIT/ANE2-GCPDS/Datasets/ANEaudios/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;66;03m# Execute the main analysis function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m \u001b[43manalyze_and_extract_ar_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_TO_AUDIO_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 170\u001b[39m, in \u001b[36manalyze_and_extract_ar_features\u001b[39m\u001b[34m(directory)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# Iterate through the audio chunks provided by the generator.\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m audio_chunk \u001b[38;5;129;01min\u001b[39;00m stream_wav_file(file_path, BLOCK_SIZE):\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     chunk_features = \u001b[43mextract_ar_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAR_MODEL_LAGS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# Only include features if they were successfully extracted.\u001b[39;00m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk_features.size > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36mextract_ar_features\u001b[39m\u001b[34m(audio_chunk, lags)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# Initialize the AutoReg model. A trend component is not needed for\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# modeling the stationary properties of an audio waveform chunk.\u001b[39;00m\n\u001b[32m    119\u001b[39m     model = AutoReg(processed_chunk, lags=lags, trend=\u001b[33m'\u001b[39m\u001b[33mn\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# The model parameters (coefficients) are the extracted features.\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results.params\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GIT/GCPDS--trabajos-/Audios-AR/venv/lib/python3.13/site-packages/statsmodels/tsa/ar_model.py:479\u001b[39m, in \u001b[36mAutoReg.fit\u001b[39m\u001b[34m(self, cov_type, cov_kwds, use_t)\u001b[39m\n\u001b[32m    475\u001b[39m ols_mod = OLS(\u001b[38;5;28mself\u001b[39m._y, \u001b[38;5;28mself\u001b[39m._x)\n\u001b[32m    476\u001b[39m ols_res = ols_mod.fit(\n\u001b[32m    477\u001b[39m     cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t\n\u001b[32m    478\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m cov_params = \u001b[43mols_res\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcov_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m use_t = ols_res.use_t\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cov_type == \u001b[33m\"\u001b[39m\u001b[33mnonrobust\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_t:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GIT/GCPDS--trabajos-/Audios-AR/venv/lib/python3.13/site-packages/statsmodels/base/wrapper.py:113\u001b[39m, in \u001b[36mmake_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m     obj = data.wrap_output(func(results, *args, **kwargs), how[\u001b[32m0\u001b[39m], how[\u001b[32m1\u001b[39m:])\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m how:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     obj = data.wrap_output(\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, how)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GIT/GCPDS--trabajos-/Audios-AR/venv/lib/python3.13/site-packages/statsmodels/base/model.py:1526\u001b[39m, in \u001b[36mLikelihoodModelResults.cov_params\u001b[39m\u001b[34m(self, r_matrix, column, scale, cov_p, other)\u001b[39m\n\u001b[32m   1524\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1525\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1526\u001b[39m             scale = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\n\u001b[32m   1527\u001b[39m         cov_p = \u001b[38;5;28mself\u001b[39m.normalized_cov_params * scale\n\u001b[32m   1529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m column \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GIT/GCPDS--trabajos-/Audios-AR/venv/lib/python3.13/site-packages/statsmodels/tools/decorators.py:95\u001b[39m, in \u001b[36mCachedAttribute.__get__\u001b[39m\u001b[34m(self, obj, type)\u001b[39m\n\u001b[32m     93\u001b[39m _cachedval = _cache.get(name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cachedval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     _cachedval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     _cache[name] = _cachedval\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _cachedval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GIT/GCPDS--trabajos-/Audios-AR/venv/lib/python3.13/site-packages/statsmodels/regression/linear_model.py:1716\u001b[39m, in \u001b[36mRegressionResults.scale\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1708\u001b[39m \u001b[38;5;129m@cache_writable\u001b[39m()\n\u001b[32m   1709\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mscale\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1710\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1711\u001b[39m \u001b[33;03m    A scale factor for the covariance matrix.\u001b[39;00m\n\u001b[32m   1712\u001b[39m \n\u001b[32m   1713\u001b[39m \u001b[33;03m    The Default value is ssr/(n-p).  Note that the square root of `scale`\u001b[39;00m\n\u001b[32m   1714\u001b[39m \u001b[33;03m    is often called the standard error of the regression.\u001b[39;00m\n\u001b[32m   1715\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1716\u001b[39m     wresid = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwresid\u001b[49m\n\u001b[32m   1717\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.dot(wresid, wresid) / \u001b[38;5;28mself\u001b[39m.df_resid\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/properties.pyx:36\u001b[39m, in \u001b[36mpandas._libs.properties.CachedProperty.__get__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GIT/GCPDS--trabajos-/Audios-AR/venv/lib/python3.13/site-packages/statsmodels/regression/linear_model.py:1698\u001b[39m, in \u001b[36mRegressionResults.wresid\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1693\u001b[39m \u001b[38;5;129m@cache_readonly\u001b[39m\n\u001b[32m   1694\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwresid\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1695\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1696\u001b[39m \u001b[33;03m    The residuals of the transformed/whitened regressand and regressor(s).\u001b[39;00m\n\u001b[32m   1697\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1698\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.wendog - \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1699\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwexog\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GIT/GCPDS--trabajos-/Audios-AR/venv/lib/python3.13/site-packages/statsmodels/regression/linear_model.py:409\u001b[39m, in \u001b[36mRegressionModel.predict\u001b[39m\u001b[34m(self, params, exog)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    407\u001b[39m     exog = \u001b[38;5;28mself\u001b[39m.exog\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Audio Feature Extraction with Autoregressive Models.\n",
    "\n",
    "This script provides a memory-efficient pipeline for extracting features from a\n",
    "directory of WAV audio files. It leverages a streaming approach to process\n",
    "large files without loading them entirely into memory. For each audio chunk,\n",
    "it fits an autoregressive (AR) model using the `statsmodels` library and\n",
    "extracts the model's coefficients. These coefficients, which capture the\n",
    "spectral characteristics of the audio, are then aggregated to form a single\n",
    "feature vector for each file.\n",
    "\n",
    "The primary components are:\n",
    "  - `stream_wav_file`: A generator that reads a WAV file in chunks.\n",
    "  - `extract_ar_features`: A function to compute AR model coefficients from an\n",
    "    audio chunk.\n",
    "  - `analyze_and_extract_ar_features`: The main orchestrator that processes a\n",
    "    directory of audio files and reports a summary.\n",
    "\n",
    "This approach is suitable for preprocessing audio data for machine learning\n",
    "tasks, such as sound classification or speaker identification, where a compact\n",
    "and representative feature set is required.\n",
    "\n",
    "Example Usage:\n",
    "    To run the script, ensure `statsmodels`, `numpy`, and `soundfile` are\n",
    "    installed (`pip install statsmodels numpy soundfile`). Then, execute the\n",
    "\n",
    "    script from the command line:\n",
    "\n",
    "        $ python your_script_name.py\n",
    "\n",
    "    The script will process all `.wav` files in the predefined directory and\n",
    "    print a summary of the results, including the extracted features for an\n",
    "    example file.\n",
    "\"\"\"\n",
    "\n",
    "# --- Constants ---\n",
    "# The number of past observations to use for the AR model. This is a critical\n",
    "# hyperparameter that determines the dimensionality of the feature vector.\n",
    "AR_MODEL_LAGS: Final[int] = 12\n",
    "\n",
    "# The size of each audio chunk to read from the file, in frames. A larger\n",
    "# size reduces I/O overhead but increases memory usage per chunk.\n",
    "BLOCK_SIZE: Final[int] = 65536\n",
    "\n",
    "\n",
    "def stream_wav_file(file_path: str, block_size: int) -> Iterator[np.ndarray]:\n",
    "    \"\"\"Lazily loads a WAV file in chunks using a generator.\n",
    "\n",
    "    This function reads a WAV file piece by piece, yielding chunks of audio\n",
    "    data as NumPy arrays. This memory-efficient approach is ideal for large\n",
    "    audio files that may not fit into system RAM.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The absolute or relative path to the WAV file.\n",
    "        block_size (int): The number of audio frames to read per chunk.\n",
    "\n",
    "    Yields:\n",
    "        np.ndarray: A chunk of the audio file's waveform data.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified `file_path` does not exist.\n",
    "        sf.SoundFileError: If the file is not a valid WAV file, is corrupted,\n",
    "                           or cannot be opened.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with sf.SoundFile(file_path, 'r') as audio_file:\n",
    "            while audio_file.tell() < audio_file.frames:\n",
    "                chunk = audio_file.read(block_size)\n",
    "                # Stop if the read operation returns an empty array.\n",
    "                if not chunk.size:\n",
    "                    break\n",
    "                yield chunk\n",
    "    except (FileNotFoundError, sf.SoundFileError) as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        # Re-raise the exception to be handled by the calling function.\n",
    "        raise\n",
    "\n",
    "\n",
    "def extract_ar_features(audio_chunk: np.ndarray, lags: int) -> np.ndarray:\n",
    "    \"\"\"Extracts features from an audio chunk using an AutoReg model.\n",
    "\n",
    "    This function fits an autoregressive model to the provided audio chunk and\n",
    "    returns the model's coefficients. These coefficients serve as a compact\n",
    "    representation of the chunk's spectral envelope.\n",
    "\n",
    "    Args:\n",
    "        audio_chunk (np.ndarray): The audio data chunk, which can be mono\n",
    "                                  (1D) or stereo (2D).\n",
    "        lags (int): The number of autoregressive lags (the model order) to use\n",
    "                    for the feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 1D NumPy array containing the AR model coefficients.\n",
    "                    Returns an empty array if the model fitting fails or if\n",
    "                    the chunk is too small for the specified number of lags.\n",
    "    \"\"\"\n",
    "    processed_chunk = audio_chunk\n",
    "    # AutoReg models require a 1D time series. If the audio is stereo,\n",
    "    # convert it to mono by averaging the channels.\n",
    "    if processed_chunk.ndim > 1:\n",
    "        processed_chunk = np.mean(processed_chunk, axis=1)\n",
    "\n",
    "    # The model cannot be fitted if the number of samples is not greater\n",
    "    # than the number of lags.\n",
    "    if len(processed_chunk) <= lags:\n",
    "        return np.array([])\n",
    "\n",
    "    try:\n",
    "        # Initialize the AutoReg model. A trend component is not needed for\n",
    "        # modeling the stationary properties of an audio waveform chunk.\n",
    "        model = AutoReg(processed_chunk, lags=lags, trend='n')\n",
    "        results = model.fit()\n",
    "\n",
    "        # The model parameters (coefficients) are the extracted features.\n",
    "        return results.params\n",
    "    except (ValueError, InfeasibleTestError):\n",
    "        # This block catches potential errors from statsmodels, for example,\n",
    "        # if the data chunk contains constant values (e.g., silence).\n",
    "        return np.array([])\n",
    "\n",
    "\n",
    "def analyze_and_extract_ar_features(directory: str) -> None:\n",
    "    \"\"\"Scans a directory, extracts AR features from WAV files, and reports results.\n",
    "\n",
    "    This function orchestrates the entire feature extraction process. It locates\n",
    "    all `.wav` files in the specified directory, streams each file chunk by\n",
    "    chunk, computes autoregressive features, and aggregates them into a single\n",
    "    feature vector per file.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The path to the directory containing the `.wav` files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        wav_files = sorted([\n",
    "            f for f in os.listdir(directory) if f.lower().endswith(\".wav\")\n",
    "        ])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The directory '{directory}' was not found.\")\n",
    "        return\n",
    "\n",
    "    if not wav_files:\n",
    "        print(f\"No .wav files found in the directory: '{directory}'\")\n",
    "        return\n",
    "\n",
    "    # This dictionary will store the final feature vector for each file.\n",
    "    all_file_features: Dict[str, np.ndarray] = {}\n",
    "    total_processing_time = 0.0\n",
    "\n",
    "    print(\"--- Processing Audio Files and Extracting AutoReg Features ---\")\n",
    "    print(f\"Model Order (lags): {AR_MODEL_LAGS}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for file_name in wav_files:\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        features_for_current_file: List[np.ndarray] = []\n",
    "\n",
    "        try:\n",
    "            start_time = time.perf_counter()\n",
    "\n",
    "            # Iterate through the audio chunks provided by the generator.\n",
    "            for audio_chunk in stream_wav_file(file_path, BLOCK_SIZE):\n",
    "                chunk_features = extract_ar_features(audio_chunk, lags=AR_MODEL_LAGS)\n",
    "\n",
    "                # Only include features if they were successfully extracted.\n",
    "                if chunk_features.size > 0:\n",
    "                    features_for_current_file.append(chunk_features)\n",
    "\n",
    "            end_time = time.perf_counter()\n",
    "            processing_time = end_time - start_time\n",
    "            total_processing_time += processing_time\n",
    "\n",
    "            if features_for_current_file:\n",
    "                # Aggregate features by taking the element-wise mean of the\n",
    "                # feature vectors from all chunks of the file.\n",
    "                aggregated_features = np.mean(features_for_current_file, axis=0)\n",
    "                all_file_features[file_name] = aggregated_features\n",
    "                print(f\"Processed: {file_name:<35} | Time: {processing_time:>8.4f}s\")\n",
    "            else:\n",
    "                print(f\"Warning: Could not extract any valid features from {file_name}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch any other unexpected errors during file processing.\n",
    "            print(f\"An unexpected error occurred while processing {file_name}: {e}\")\n",
    "\n",
    "    # --- Summary of Results ---\n",
    "    print(\"-\" * 60)\n",
    "    print(\"\\n--- Feature Extraction Summary ---\")\n",
    "    print(f\"Total files successfully processed: {len(all_file_features)}\")\n",
    "    print(f\"Total processing time: {total_processing_time:.4f} seconds\")\n",
    "\n",
    "    # Display the extracted features for the first processed file as an example.\n",
    "    if all_file_features:\n",
    "        first_file = list(all_file_features.keys())[0]\n",
    "        print(f\"\\nExample aggregated AR coefficients for '{first_file}':\")\n",
    "        print(all_file_features[first_file])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the target directory containing the audio files.\n",
    "    # IMPORTANT: Modify this path to point to your dataset.\n",
    "    PATH_TO_AUDIO_DIR = \"/home/javastral/GIT/ANE2-GCPDS/Datasets/ANEaudios/\"\n",
    "\n",
    "    # Execute the main analysis function.\n",
    "    analyze_and_extract_ar_features(PATH_TO_AUDIO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d486ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1205, 83886)\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/javastral/GIT/ANE2-GCPDS/Datasets/ANEaudios/\"\n",
    "wav_files = [file for file in os.listdir(path) if file.endswith(\".wav\")]\n",
    "wav_files.sort()\n",
    "\n",
    "audio_vectors = []\n",
    "total_files = len(wav_files)\n",
    "\n",
    "for i in range(total_files):\n",
    "    filepath = os.path.join(path, wav_files[i])\n",
    "    data, _ = sf.read(filepath)\n",
    "    audio_vectors.append(data)\n",
    "\n",
    "audio_vectors = np.array(audio_vectors)\n",
    "print(audio_vectors.shape)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
