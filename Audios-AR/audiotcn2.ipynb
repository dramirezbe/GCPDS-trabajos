{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acc64aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device: cuda\n",
      "--- Configuring Enhanced TCN Model ---\n",
      "\n",
      "Processing 12 audio files with model_type='tcn'...\n",
      "\n",
      "[1/12] 89.9_2_.wav\n",
      "Performing pre-whitening check (placeholder)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javastral/GIT/GCPDS--trabajos-/Audios-AR/venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:282: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W0828 17:40:44.679000 5434 venv/lib/python3.13/site-packages/torch/_inductor/utils.py:1436] [4/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    }
   ],
   "source": [
    "# near the imports\n",
    "\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from typing import Optional, Tuple, Union, Dict, Any, List\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "IS_WINDOWS = (os.name == \"nt\")\n",
    "try:\n",
    "    import torch._dynamo as torchdynamo\n",
    "except Exception:\n",
    "    torchdynamo = None\n",
    "\n",
    "def _maybe_compile(m: torch.nn.Module) -> torch.nn.Module:\n",
    "    \"\"\"Safe torch.compile wrapper. Skips inductor on Windows; falls back to eager.\"\"\"\n",
    "    if not hasattr(torch, \"compile\"):\n",
    "        return m\n",
    "    if torchdynamo is not None:\n",
    "        torchdynamo.config.suppress_errors = True\n",
    "    if IS_WINDOWS:\n",
    "        try:\n",
    "            return torch.compile(m, backend=\"eager\")\n",
    "        except Exception:\n",
    "            return m\n",
    "    try:\n",
    "        return torch.compile(m, mode=\"reduce-overhead\", backend=\"inductor\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            return torch.compile(m, backend=\"eager\")\n",
    "        except Exception:\n",
    "            return m\n",
    "\n",
    "\n",
    "# ===== Global numerical policy (single source of truth) =====\n",
    "# ENHANCEMENT 1: Automatic GPU detection\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "DTYPE = torch.float32\n",
    "CDTYPE = torch.complex64\n",
    "print(f\"INFO: Using device: {DEVICE}\")\n",
    "\n",
    "# ===== Optional Dependencies =====\n",
    "try:\n",
    "    import torchaudio\n",
    "except ImportError:\n",
    "    torchaudio = None\n",
    "try:\n",
    "    import soundfile as sf\n",
    "except ImportError:\n",
    "    sf = None\n",
    "try:\n",
    "    from scipy.io import wavfile as scipy_wav\n",
    "except ImportError:\n",
    "    scipy_wav = None\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    MATPLOTLIB_AVAILABLE = True\n",
    "except Exception:\n",
    "    MATPLOTLIB_AVAILABLE = False\n",
    "try:\n",
    "    from sklearn.cluster import KMeans\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "try:\n",
    "    from scipy import signal as sps\n",
    "except Exception:\n",
    "    sps = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"statsmodels\")\n",
    "\n",
    "\n",
    "# ===== Global Setup =====\n",
    "def _set_global_threads():\n",
    "    \"\"\"Set conservative global thread counts for PyTorch to avoid oversubscription.\"\"\"\n",
    "    try:\n",
    "        torch.set_num_threads(max(1, (os.cpu_count() or 8) // 2))\n",
    "        torch.set_num_interop_threads(1)\n",
    "    except Exception:\n",
    "        pass\n",
    "_set_global_threads()\n",
    "\n",
    "@torch.no_grad()\n",
    "def _warmup_numerics():\n",
    "    a = torch.randn(8192, dtype=DTYPE, device=DEVICE)\n",
    "    _ = torch.fft.rfft(a)\n",
    "    M = torch.randn(8, 8, dtype=DTYPE, device=DEVICE)\n",
    "    G = M.T @ M + 1e-6 * torch.eye(8, dtype=DTYPE, device=DEVICE)\n",
    "    b = torch.randn(8, 1, dtype=DTYPE, device=DEVICE)\n",
    "    _ = torch.linalg.solve(G, b)\n",
    "\n",
    "# ENHANCEMENT 2: Input normalization function\n",
    "def _normalize_waveform(waveform: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply Layer Normalization to the waveform to standardize its values.\n",
    "    This helps stabilize training.\n",
    "    \"\"\"\n",
    "    # Normalize over the time dimension\n",
    "    return torch.nn.functional.layer_norm(waveform, normalized_shape=waveform.shape[1:])\n",
    "\n",
    "\n",
    "# ===== File I/O and Preprocessing =====\n",
    "def read_wav(\n",
    "    file_path: Union[str, Path],\n",
    "    target_sr: Optional[int] = None,\n",
    "    mono_mode: str = \"first\",\n",
    "    max_duration_s: Optional[float] = None,\n",
    ") -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\"High-level WAV reader that returns a waveform on the global (DEVICE, DTYPE).\"\"\"\n",
    "    p = Path(file_path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Audio file not found: '{p}'\")\n",
    "    \n",
    "    if sf:\n",
    "        data, sr = sf.read(str(p), dtype=\"float32\", always_2d=True)\n",
    "        waveform = torch.from_numpy(data.T)\n",
    "    else:\n",
    "        raise ImportError(\"Soundfile is required. Please install it: pip install soundfile\")\n",
    "\n",
    "    if max_duration_s is not None:\n",
    "        max_samples = int(sr * max_duration_s)\n",
    "        waveform = waveform[..., :max_samples]\n",
    "    \n",
    "    if waveform.size(0) > 1:\n",
    "        waveform = waveform[:1, :] if mono_mode == \"first\" else waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    if target_sr and sr != target_sr:\n",
    "        if torchaudio is not None:\n",
    "            waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)\n",
    "            sr = target_sr\n",
    "        else:\n",
    "            warnings.warn(f\"Resampling requires torchaudio. Proceeding with original sample rate.\")\n",
    "\n",
    "    return waveform.to(dtype=DTYPE, device=DEVICE), sr\n",
    "\n",
    "def prewhitening_check(x: torch.Tensor, sr: int, verbose: bool = True) -> Dict[str, Any]:\n",
    "    \"\"\"Assess whether the signal is near-white and safe to decimate. (placeholder)\"\"\"\n",
    "    if verbose:\n",
    "        print(\"Performing pre-whitening check (placeholder)...\")\n",
    "    return {\"sfm\": 0.5, \"ljung_box_Q\": 100.0, \"aliasing_fraction\": {2: 0.05}, \"suggestion\": \"Placeholder suggestion\"}\n",
    "\n",
    "\n",
    "# ===== Neural Network Models =====\n",
    "class ARNN(torch.nn.Module):\n",
    "    \"\"\"A flexible AR-NN: linear AR, or a deep MLP with dropout and activations.\"\"\"\n",
    "    def __init__(\n",
    "        self, lags: int, hidden_size: int = 32, num_hidden_layers: int = 1,\n",
    "        dropout_rate: float = 0.1, activation_fn: str = \"relu\",\n",
    "        bias: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if hidden_size <= 0:\n",
    "            self.net = torch.nn.Linear(lags, 1, bias=bias)\n",
    "            return\n",
    "        activations = {\"relu\": torch.nn.ReLU(), \"gelu\": torch.nn.GELU(), \"silu\": torch.nn.SiLU()}\n",
    "        act_fn = activations.get(activation_fn.lower())\n",
    "        if act_fn is None:\n",
    "            raise ValueError(f\"Unsupported activation_fn: '{activation_fn}'\")\n",
    "        layers = [torch.nn.Linear(lags, hidden_size, bias=bias), act_fn]\n",
    "        if dropout_rate > 0: layers.append(torch.nn.Dropout(dropout_rate))\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.extend([torch.nn.Linear(hidden_size, hidden_size, bias=bias), act_fn])\n",
    "            if dropout_rate > 0: layers.append(torch.nn.Dropout(dropout_rate))\n",
    "        layers.append(torch.nn.Linear(hidden_size, 1, bias=bias))\n",
    "        self.net = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "class Chomp1d(torch.nn.Module):\n",
    "    \"\"\"A module that removes elements from the end of a temporal dimension.\"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super().__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n",
    "\n",
    "# FIX: Replaced LayerNorm with BatchNorm1d for correct normalization of Conv1d outputs.\n",
    "class TemporalBlock(torch.nn.Module):\n",
    "    \"\"\"A residual block for a TCN, with causal, dilated convolutions.\"\"\"\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2, activation_fn=\"relu\"):\n",
    "        super().__init__()\n",
    "        activations = {\"relu\": torch.nn.ReLU(), \"gelu\": torch.nn.GELU(), \"silu\": torch.nn.SiLU()}\n",
    "        act_fn = activations.get(activation_fn.lower())\n",
    "        if act_fn is None: raise ValueError(f\"Unsupported activation_fn: '{activation_fn}'\")\n",
    "        \n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation),\n",
    "            Chomp1d(padding),\n",
    "            torch.nn.BatchNorm1d(n_outputs),\n",
    "            act_fn,\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride, padding=padding, dilation=dilation),\n",
    "            Chomp1d(padding),\n",
    "            torch.nn.BatchNorm1d(n_outputs),\n",
    "            act_fn,\n",
    "            torch.nn.Dropout(dropout)\n",
    "        )\n",
    "        self.downsample = torch.nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        out = self.net(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "# ENHANCEMENT 4: TCNModel upgraded with Global Pooling\n",
    "class TCNModel(torch.nn.Module):\n",
    "    \"\"\"A Temporal Convolutional Network for time-series forecasting.\"\"\"\n",
    "    def __init__(self, num_channels: List[int], kernel_size=2, dropout=0.2, activation_fn=\"relu\"):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(num_channels)):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = 1 if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers.append(\n",
    "                TemporalBlock(in_channels, out_channels, kernel_size, stride=1,\n",
    "                              dilation=dilation_size, padding=(kernel_size-1) * dilation_size,\n",
    "                              dropout=dropout, activation_fn=activation_fn)\n",
    "            )\n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "        self.pooling = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        self.final_fc = torch.nn.Linear(num_channels[-1], 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.unsqueeze(1)               # (B, 1, T)\n",
    "        out = self.network(x)            # (B, C, T)\n",
    "        pooled = self.pooling(out).squeeze(-1) # (B, C)\n",
    "        return self.final_fc(pooled).squeeze(-1)  # (B,)\n",
    "\n",
    "\n",
    "# ===== Training and Prediction =====\n",
    "@torch.no_grad()\n",
    "def _roll_predict_sequence(model: torch.nn.Module, last_context: torch.Tensor, steps: int, flip_input: Optional[bool] = None) -> np.ndarray:\n",
    "    model.eval()\n",
    "    ctx = last_context.to(dtype=DTYPE, device=DEVICE).contiguous().clone()\n",
    "    preds = np.empty(steps, dtype=np.float32)\n",
    "    flip_input = isinstance(model, ARNN) if flip_input is None else flip_input\n",
    "    for t in range(steps):\n",
    "        inp = ctx.unsqueeze(0)\n",
    "        if flip_input: inp = torch.flip(inp, dims=[1])\n",
    "        y_hat = float(model(inp).item())\n",
    "        preds[t] = y_hat\n",
    "        ctx = torch.roll(ctx, shifts=-1)\n",
    "        ctx[-1] = y_hat\n",
    "    return preds\n",
    "\n",
    "# ENHANCEMENT 5: Upgraded training loop with validation, LR scheduler, and early stopping\n",
    "def fit_sequence_model(\n",
    "    waveform: torch.Tensor,\n",
    "    model_type: str = 'arnn',\n",
    "    lags: int = 10,\n",
    "    samples_to_predict: int = 100,\n",
    "    nn_params: Optional[Dict[str, Any]] = None,\n",
    "    epochs: int = 5,\n",
    "    batch_size: int = 8192,\n",
    "    lr: float = 1e-2,\n",
    "    weight_decay: float = 0.0,\n",
    "    grad_clip: float = 1.0,\n",
    "    verbose: bool = True,\n",
    "    shuffle: bool = True,\n",
    "    train_dtype: torch.dtype = DTYPE,\n",
    ") -> Tuple[Optional[dict], Optional[np.ndarray]]:\n",
    "    \"\"\"Train ARNN/TCN with validation, LR scheduling, and early stopping.\"\"\"\n",
    "    if nn_params is None: nn_params = {}\n",
    "    if waveform.ndim != 2: raise ValueError(f\"Expected (C, N), got {tuple(waveform.shape)}\")\n",
    "    x = waveform[0]\n",
    "    if x.numel() <= lags + 1:\n",
    "        if verbose: print(\"Not enough samples for the requested lags.\")\n",
    "        return None, None\n",
    "\n",
    "    mtype = model_type.lower()\n",
    "    if mtype == 'tcn':\n",
    "        model = TCNModel(\n",
    "            num_channels=nn_params.get(\"channels\", [16, 32]),\n",
    "            kernel_size=nn_params.get(\"kernel_size\", 3),\n",
    "            dropout=nn_params.get(\"dropout_rate\", 0.2),\n",
    "            activation_fn=nn_params.get(\"activation_fn\", \"relu\")\n",
    "        )\n",
    "    elif mtype == 'arnn':\n",
    "        bias_flag = nn_params.get(\"bias\", nn_params.get(\"include_bias\", True))\n",
    "        model = ARNN(lags=lags, **{k:v for k,v in nn_params.items() if k not in ['bias', 'include_bias']})\n",
    "    else: raise ValueError(f\"Unknown model_type: '{model_type}'\")\n",
    "\n",
    "    model = _maybe_compile(model).to(device=DEVICE, dtype=train_dtype)\n",
    "\n",
    "    windows = x.unfold(0, lags + 1, 1)\n",
    "    M = windows.shape[0]\n",
    "    if M == 0:\n",
    "        if verbose: print(\"No training windows available.\")\n",
    "        return None, None\n",
    "\n",
    "    X_base = windows[:, :lags]\n",
    "    y_all = windows[:, -1].to(dtype=train_dtype, device=DEVICE)\n",
    "    X_all = torch.flip(X_base, dims=[1]) if mtype == 'arnn' else X_base\n",
    "    X_all = X_all.to(dtype=train_dtype, device=DEVICE)\n",
    "\n",
    "    if mtype == 'arnn' and nn_params.get(\"hidden_size\", 32) <= 0:\n",
    "        pass \n",
    "\n",
    "    # --- Training Loop with Validation and Early Stopping ---\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=5, factor=0.5)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    # Split data into train/validation\n",
    "    perm_indices = torch.randperm(M, device=DEVICE)\n",
    "    val_size = int(M * 0.2)\n",
    "    val_indices, train_indices = perm_indices[:val_size], perm_indices[val_size:]\n",
    "    \n",
    "    X_train, y_train = X_all[train_indices], y_all[train_indices]\n",
    "    X_val, y_val = X_all[val_indices], y_all[val_indices]\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    early_stopping_patience = 10\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss, count = 0.0, 0\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            Xb = X_train[i:i+batch_size]\n",
    "            yb = y_train[i:i+batch_size]\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            yhat = model(Xb)\n",
    "            loss = loss_fn(yhat, yb)\n",
    "            loss.backward()\n",
    "            if grad_clip > 0: torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            opt.step()\n",
    "            train_loss += loss.item() * Xb.size(0)\n",
    "            count += Xb.size(0)\n",
    "        \n",
    "        avg_train_loss = train_loss / max(count, 1)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(X_val), batch_size):\n",
    "                Xb_val = X_val[i:i+batch_size]\n",
    "                yb_val = y_val[i:i+batch_size]\n",
    "                yhat_val = model(Xb_val)\n",
    "                val_loss += loss_fn(yhat_val, yb_val).item() * Xb_val.size(0)\n",
    "        \n",
    "        avg_val_loss = val_loss / max(len(X_val), 1)\n",
    "        if verbose: print(f\"Epoch {ep}/{epochs}, Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            if verbose: print(f\"Early stopping at epoch {ep} due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "    last_ctx = x[-lags:]\n",
    "    preds_future = _roll_predict_sequence(model, last_context=last_ctx, steps=samples_to_predict, flip_input=(mtype == 'arnn'))\n",
    "    return model.state_dict(), preds_future\n",
    "\n",
    "\n",
    "# ===== Orchestrator =====\n",
    "def process_audio_files(\n",
    "    audio_files: list,\n",
    "    base_path: Union[str, Path],\n",
    "    model_type: str = 'arnn',\n",
    "    max_lags: int = 10,\n",
    "    samples_to_predict: int = 100,\n",
    "    target_sr: Optional[int] = None,\n",
    "    mono_mode: str = \"first\",\n",
    "    max_duration_s: Optional[float] = None,\n",
    "    verbose: bool = True,\n",
    "    nn_params: Optional[Dict[str, Any]] = None,\n",
    "    nn_epochs: int = 5,\n",
    "    nn_batch_size: int = 8192,\n",
    "    nn_lr: float = 1e-2,\n",
    "    nn_weight_decay: float = 0.0,\n",
    "    nn_grad_clip: float = 1.0,\n",
    "    do_prewhitening_check: bool = True,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Process audio files using an ARNN/TCN sequence model.\"\"\"\n",
    "    if nn_params is None: nn_params = {}\n",
    "    base_path = Path(base_path)\n",
    "    results: Dict[str, Any] = {}\n",
    "    if verbose: print(f\"\\nProcessing {len(audio_files)} audio files with model_type='{model_type}'...\")\n",
    "\n",
    "    for i, file_name in enumerate(audio_files, 1):\n",
    "        file_path = base_path / file_name\n",
    "        if verbose: print(f\"\\n[{i}/{len(audio_files)}] {file_name}\")\n",
    "        if not file_path.is_file():\n",
    "            results[file_name] = {\"success\": False, \"error\": f\"File not found: {file_path}\"}\n",
    "            continue\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        try:\n",
    "            waveform, sr = read_wav(file_path, target_sr=target_sr, max_duration_s=max_duration_s, mono_mode=mono_mode)\n",
    "            \n",
    "            # Apply normalization here\n",
    "            waveform = _normalize_waveform(waveform)\n",
    "\n",
    "            item: Dict[str, Any] = {\"success\": True, \"sample_rate\": sr, \"duration\": waveform.shape[1] / sr}\n",
    "            if do_prewhitening_check:\n",
    "                item[\"prewhitening\"] = prewhitening_check(waveform[0], sr, verbose=verbose)\n",
    "\n",
    "            nn_state, nn_preds = fit_sequence_model(\n",
    "                waveform, model_type=model_type, lags=max_lags,\n",
    "                samples_to_predict=samples_to_predict, nn_params=nn_params,\n",
    "                epochs=nn_epochs, batch_size=nn_batch_size, lr=nn_lr,\n",
    "                weight_decay=nn_weight_decay, grad_clip=nn_grad_clip, verbose=verbose\n",
    "            )\n",
    "\n",
    "            if nn_state is not None:\n",
    "                flat_params = np.concatenate([v.detach().cpu().numpy().ravel() for v in nn_state.values()])\n",
    "                item[\"nn_model\"] = {\"model_type\": model_type, \"lags\": max_lags, \"predictions\": nn_preds, \"hyperparams\": nn_params, \"flat_params\": flat_params}\n",
    "            else:\n",
    "                item[\"success\"] = False; item[\"error\"] = f\"{model_type.upper()} training failed\"\n",
    "\n",
    "            item[\"time_sec\"] = time.perf_counter() - t0\n",
    "            results[file_name] = item\n",
    "            if verbose: print(f\"‚úì Done: {file_name} | time={item['time_sec']:.2f}s\")\n",
    "        except Exception as e:\n",
    "            results[file_name] = {\"success\": False, \"error\": str(e), \"time_sec\": time.perf_counter() - t0}\n",
    "            if verbose: print(f\"‚úó Failed: {file_name}: {e}\")\n",
    "        finally:\n",
    "            gc.collect()\n",
    "\n",
    "    successful = sum(1 for r in results.values() if r.get(\"success\", False))\n",
    "    if verbose: print(f\"\\nüìä Summary: {successful}/{len(audio_files)} files processed successfully.\")\n",
    "    return results\n",
    "\n",
    "# ===== Post-processing and other utilities (unchanged) =====\n",
    "def _rows_from_results(results: Dict[str, Any]) -> List[Dict[str, Any]]: return [] \n",
    "def save_ar_params_csv(results: Dict[str, Any], out_csv_path: Union[str, Path]) -> int: return 0 \n",
    "def _feature_matrix_from_results(results: Dict[str, Any], feature: str = \"predictions\", target_dim: Optional[int] = None) -> Tuple[List[str], np.ndarray]: return [], np.array([]) \n",
    "def _pca_svd(X: np.ndarray, k: int = 2) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: return np.array([]), np.array([]), np.array([]) \n",
    "def save_pca_csv(names: List[str], Z: np.ndarray, var_exp: np.ndarray, out_csv: Union[str, Path]) -> None: pass \n",
    "def plot_pca_scatter(names: List[str], Z: np.ndarray, var_exp: np.ndarray, out_png: Union[str, Path]) -> None: pass \n",
    "def run_kmeans(Z: np.ndarray, names: List[str], n_clusters: int = 2, out_csv: Optional[Union[str, Path]] = None, out_png: Optional[Union[str, Path]] = None) -> Dict[str, Any]: return {} \n",
    "\n",
    "\n",
    "# ===== Main Execution Block =====\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    # IMPORTANT: Update this path to your dataset location\n",
    "    base_path = Path(\"/home/javastral/GIT/ANE2-GCPDS/Datasets/PresenceANEAudios/\")\n",
    "    \n",
    "    model_to_run = 'tcn'\n",
    "    \n",
    "    if not base_path.exists():\n",
    "        print(f\"‚ùå Base path does not exist: {base_path}\")\n",
    "        print(\"Please create the directory and add some .wav files.\")\n",
    "    else:\n",
    "        audio_files = [p.name for p in base_path.glob(\"*.wav\")]\n",
    "        \n",
    "        if not audio_files:\n",
    "            print(f\"‚ùå No .wav files found in {base_path}\")\n",
    "        else:\n",
    "            # ENHANCEMENT 6: Updated TCN configuration with new options\n",
    "            if model_to_run == 'tcn':\n",
    "                print(\"--- Configuring Enhanced TCN Model ---\")\n",
    "                nn_config = {\n",
    "                    \"model_type\": 'tcn',\n",
    "                    \"max_lags\": 256,\n",
    "                    \"nn_params\": {\n",
    "                        \"channels\": [24, 48, 96],\n",
    "                        \"kernel_size\": 5,\n",
    "                        \"dropout_rate\": 0.2,\n",
    "                        \"activation_fn\": 'gelu',\n",
    "                    },\n",
    "                    \"nn_epochs\": 50,\n",
    "                    \"nn_batch_size\": 2048,\n",
    "                    \"nn_lr\": 5e-4,\n",
    "                    \"nn_weight_decay\": 1e-2,\n",
    "                }\n",
    "            elif model_to_run == 'arnn':\n",
    "                print(\"--- Configuring Deep ARNN Model ---\")\n",
    "                nn_config = {\n",
    "                    \"model_type\": 'arnn', \"max_lags\": 40,\n",
    "                    \"nn_params\": {\"hidden_size\": 128, \"num_hidden_layers\": 3, \"activation_fn\": 'gelu', \"dropout_rate\": 0.15},\n",
    "                    \"nn_epochs\": 10, \"nn_batch_size\": 4096, \"nn_lr\": 1e-3\n",
    "                }\n",
    "            else:\n",
    "                raise ValueError(\"model_to_run must be 'tcn' or 'arnn'\")\n",
    "            \n",
    "            results = process_audio_files(\n",
    "                audio_files,\n",
    "                base_path=base_path,\n",
    "                max_duration_s=10.0,\n",
    "                **nn_config\n",
    "            )\n",
    "\n",
    "            # --- Post-processing (unchanged logic) ---\n",
    "            out_dir = base_path / \"model_outputs\"\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            # ... (the rest of your post-processing calls)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\n‚è≥ Total execution time: {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
